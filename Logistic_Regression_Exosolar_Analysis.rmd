---
title: "Logistic_Regression_Exosolar_Analysis"
author: "Niket"
output: rmarkdown::github_document
---

Logistic Regression Model - Logistic regression predictions are discrete values(Life or no Life). ITthe output is in the form of probabilities of the default class. As it is a probability, the output lies in the range of 0-1. The output y-value is generated by log transforming the x-value. Then we force this probability into a binary classification.


```{r}
#install.packages('caret')
require(caret)

#install.packages('e1071')
require(e1071)
```

```{r}
# load the new dataset
exosim <- read.csv(file.choose(), header = TRUE) # load exos_new-imp2.csv
```

```{r}
#### Converting variable Probability_of_life to factor
exosim$Probability_of_life <- as.factor(exosim$Probability_of_life)
table(exosim$Probability_of_life)
# So we have 328 planets in our dataset with some probability of life.
```

```{r}
label_1 <- exosim$Probability_of_life # our variable for classification
```

```{r}
set.seed(1234)
oneortwo <- sample(1:2 , length(exosim$PlanetIdentifier), replace = TRUE, prob=c(0.8, 0.2)) # generating random values and storing them
```

```{r}
# create train data frame
train_2 <- exosim[oneortwo == 1, -26]

# create test data frame
test_2 <- exosim[oneortwo == 2, -26]

# create data frame to apply train and test upon
train_2_label <- label_1[oneortwo == 1]
test_2_label <- label_1[oneortwo == 2]
```


```{r}
test_2 <- data.frame(test_2, test_2_label)
head(test_2)

train_2 <- data.frame(train_2, train_2_label)
head(train_2)
```

#### Logistic Regression

```{r}
life_predicted_5 <- glm(train_2_label ~ PlanetaryMassJpt + RadiusJpt + PeriodDays + SemiMajorAxisAU + Eccentricity +  SurfaceTempK + AgeGyr + DiscoveryYear + DistFromSunParsec + HostStarMassSlrMass + HostStarRadiusSlrRad + HostStarMetallicity + HostStarTempK + HostStarAgeGyr  , family = binomial(link='logit'), data = train_2, control = list(maxit = 50))
```

```{r}
prediction_5 <- predict(life_predicted_5, test_2, type = 'response')  # type='response', R will output probabilities in the form of P(y=1|X).
summary(prediction_5)
# the observation is not (1,0)

prediction_5 <- as.numeric(prediction_5 > 0.5, 1, 0) 
# this step to convert the observation to be classified as 1 and 0 otherwise . We take decision boundary to be 0.5.

mean(as.numeric(prediction_5 > 0.5) != test_2$test_2_label) # test error
# test error = 0.08  is pretty low

results_5 <- data.frame(prediction_5, test_2$test_2_label)

accuracy_5  <- paste("Accuracy of Logistic Regression Model is:", sum(prediction_5 == test_2$test_2_label)/length(prediction_5))
logisticregression <- sum(prediction_5 == test_2$test_2_label)/length(prediction_5)

confusionMatrix(table(results_5))
```

We get an accuracy of 91.2% which is less compred to other models, but it outperformed NaiveBayes. Usually logistic regression performs good for binary classification but with our variables it gives less accuracy compared to other models.

***